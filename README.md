# Spark å­¦ä¹ æ¼”ç¤ºé¡¹ç›®

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ Apache Spark å­¦ä¹ é¡¹ç›®ï¼Œä½¿ç”¨ Java 21 å¼€å‘ï¼Œæ¶µç›–äº† Spark çš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µå’Œé«˜çº§ç‰¹æ€§ã€‚é¡¹ç›®åŒ…å«äº†ä¸°å¯Œçš„ç¤ºä¾‹ä»£ç å’Œè¯¦ç»†çš„æ³¨é‡Šï¼Œé€‚åˆåˆå­¦è€…å­¦ä¹ å’Œè¿›é˜¶å¼€å‘è€…å‚è€ƒã€‚

## æŠ€æœ¯æ ˆ

- **Java**: 21
- **Spark**: 3.5.1
- **Scala**: 2.12
- **Hadoop**: 3.3.6
- **Maven**: é¡¹ç›®ç®¡ç†å’Œæ„å»ºå·¥å…·
- **Docker**: å®¹å™¨åŒ–éƒ¨ç½²

## é¡¹ç›®ç»“æ„

```
spark/
â”œâ”€â”€ pom.xml                                    # Maven é¡¹ç›®é…ç½®
â”œâ”€â”€ docker-compose.yml                         # Docker Compose é…ç½®
â”œâ”€â”€ README.md                                  # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ .gitignore                                 # Git å¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ java/
â”‚       â”‚   â””â”€â”€ com/
â”‚       â”‚       â””â”€â”€ spark/
â”‚       â”‚           â””â”€â”€ learning/
â”‚       â”‚               â”œâ”€â”€ SparkLearningMain.java              # ä¸»ç¨‹åºå…¥å£
â”‚       â”‚               â””â”€â”€ demo/
â”‚       â”‚                   â”œâ”€â”€ RDDOperationsDemo.java          # RDD æ“ä½œæ¼”ç¤º
â”‚       â”‚                   â”œâ”€â”€ DataFrameAndDatasetDemo.java    # DataFrame/Dataset æ¼”ç¤º
â”‚       â”‚                   â”œâ”€â”€ SparkSQLDemo.java               # Spark SQL æ¼”ç¤º
â”‚       â”‚                   â”œâ”€â”€ DataIODemo.java                 # æ•°æ®è¯»å†™æ¼”ç¤º
â”‚       â”‚                   â””â”€â”€ AdvancedFeaturesDemo.java       # é«˜çº§ç‰¹æ€§æ¼”ç¤º
â”‚       â””â”€â”€ resources/
â”‚           â””â”€â”€ log4j.properties               # æ—¥å¿—é…ç½®
â””â”€â”€ data/                                      # æ•°æ®æ–‡ä»¶ç›®å½•
    â”œâ”€â”€ input/                                 # è¾“å…¥æ•°æ®
    â””â”€â”€ output/                                # è¾“å‡ºæ•°æ®
```

## æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. RDD æ“ä½œ (RDDOperationsDemo)

- **RDD åˆ›å»º**: ä»é›†åˆã€æ–‡ä»¶ã€å…¶ä»– RDD åˆ›å»º
- **è½¬æ¢æ“ä½œ**: map, filter, flatMap, distinct, union, intersection, subtract
- **è¡ŒåŠ¨æ“ä½œ**: collect, count, first, take, reduce, fold, aggregate, foreach
- **Pair RDD æ“ä½œ**: reduceByKey, groupByKey, mapValues, join, cogroup, sortByKey
- **åˆ†åŒºæ“ä½œ**: repartition, coalesce, mapPartitions, mapPartitionsWithIndex
- **æŒä¹…åŒ–**: cache, persist, unpersist

### 2. DataFrame å’Œ Dataset (DataFrameAndDatasetDemo)

- **åˆ›å»ºæ–¹å¼**: ä»é›†åˆã€Bean ç±»ã€RDD åˆ›å»º
- **åŸºæœ¬æ“ä½œ**: select, filter, where, distinct, orderBy, limit
- **åˆ—æ“ä½œ**: withColumn, withColumnRenamed, drop, dropDuplicates
- **èšåˆæ“ä½œ**: groupBy, agg, count, avg, max, min, sum
- **Join æ“ä½œ**: inner, left, right, full, cross join
- **UDF**: ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°

### 3. Spark SQL (SparkSQLDemo)

- **åŸºæœ¬æŸ¥è¯¢**: SELECT, WHERE, ORDER BY, LIMIT, DISTINCT
- **èšåˆå‡½æ•°**: COUNT, SUM, AVG, MAX, MIN, STDDEV
- **JOIN æ“ä½œ**: INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN
- **å­æŸ¥è¯¢**: WHERE å­æŸ¥è¯¢, FROM å­æŸ¥è¯¢, IN å­æŸ¥è¯¢
- **çª—å£å‡½æ•°**: ROW_NUMBER, RANK, DENSE_RANK, NTILE, LAG, LEAD
- **å¤æ‚æŸ¥è¯¢**: CASE WHEN, WITH (CTE), UNION
- **è§†å›¾ç®¡ç†**: ä¸´æ—¶è§†å›¾ã€å…¨å±€ä¸´æ—¶è§†å›¾

### 4. æ•°æ®è¯»å†™ (DataIODemo)

- **æ–‡ä»¶æ ¼å¼**:
  - CSV: æ–‡æœ¬æ ¼å¼ï¼Œæ˜“è¯»ä½†æ€§èƒ½è¾ƒä½
  - JSON: åŠç»“æ„åŒ–æ•°æ®ï¼Œçµæ´»ä½†å ç”¨ç©ºé—´å¤§
  - Parquet: åˆ—å¼å­˜å‚¨ï¼Œé«˜æ•ˆå‹ç¼©ï¼ŒSpark æ¨èæ ¼å¼
  - ORC: ä¼˜åŒ–çš„è¡Œåˆ—å¼æ–‡ä»¶æ ¼å¼
  - Text: çº¯æ–‡æœ¬æ–‡ä»¶
- **ä¿å­˜æ¨¡å¼**: Overwrite, Append, ErrorIfExists, Ignore
- **åˆ†åŒºå†™å…¥**: partitionBy
- **è¯»å–é€‰é¡¹**: header, inferSchema, sep, dateFormat ç­‰
- **æ ¼å¼è½¬æ¢**: CSV â†” Parquet â†” JSON

### 5. é«˜çº§ç‰¹æ€§ (AdvancedFeaturesDemo)

- **å¹¿æ’­å˜é‡**: é«˜æ•ˆåˆ†å‘å¤§å‹åªè¯»æ•°æ®åˆ°å„ä¸ªèŠ‚ç‚¹
- **ç´¯åŠ å™¨**: å®ç°åˆ†å¸ƒå¼è®¡æ•°å™¨å’Œæ±‚å’Œ
- **çª—å£å‡½æ•°**: 
  - æ’åå‡½æ•°: rank, dense_rank, row_number
  - åˆ†æå‡½æ•°: lag, lead
  - èšåˆçª—å£: ç§»åŠ¨å¹³å‡ã€ç´¯è®¡æ±‚å’Œ
- **ç¼“å­˜å’ŒæŒä¹…åŒ–**: ä¼˜åŒ–é‡å¤è®¡ç®—
- **åˆ†åŒºä¼˜åŒ–**: repartition, coalesce, è‡ªå®šä¹‰åˆ†åŒºå™¨
- **æ•°æ®å€¾æ–œå¤„ç†**: åŠ ç› (Salting)ã€ä¸¤é˜¶æ®µèšåˆ

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šæœ¬åœ°è¿è¡Œï¼ˆæ¨èä½¿ç”¨è„šæœ¬ï¼‰

#### å‰ç½®è¦æ±‚

- JDK 21
- Maven 3.6+
- Git

#### æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
   ```bash
   git clone <repository-url>
   cd spark
   ```

2. **ä½¿ç”¨è„šæœ¬è¿è¡Œï¼ˆæ¨èï¼‰**
   ```bash
   # èµ‹äºˆæ‰§è¡Œæƒé™ï¼ˆé¦–æ¬¡ï¼‰
   chmod +x run.sh
   
   # è¿è¡Œç¨‹åº
   ./run.sh
   ```

3. **æ‰‹åŠ¨è¿è¡Œ**
   
   å¦‚æœä¸ä½¿ç”¨è„šæœ¬ï¼Œéœ€è¦æ·»åŠ  Java æ¨¡å—å‚æ•°ï¼š
   
   ```bash
   # ç¼–è¯‘é¡¹ç›®
   mvn clean package
   
   # è¿è¡Œç¨‹åºï¼ˆJava 17+ï¼‰
   java --add-exports java.base/sun.nio.ch=ALL-UNNAMED \
        --add-opens java.base/java.lang=ALL-UNNAMED \
        --add-opens java.base/java.lang.reflect=ALL-UNNAMED \
        --add-opens java.base/java.io=ALL-UNNAMED \
        --add-opens java.base/java.util=ALL-UNNAMED \
        -jar target/spark-learning-demo-1.0-SNAPSHOT.jar
   ```

4. **é€‰æ‹©æ¼”ç¤º**
   ç¨‹åºä¼šæ˜¾ç¤ºäº¤äº’å¼èœå•ï¼Œé€‰æ‹©è¦è¿è¡Œçš„æ¼”ç¤ºï¼š
   - 1: RDD æ“ä½œæ¼”ç¤º
   - 2: DataFrame å’Œ Dataset æ“ä½œæ¼”ç¤º
   - 3: Spark SQL æ¼”ç¤º
   - 4: æ•°æ®è¯»å†™æ¼”ç¤º
   - 5: é«˜çº§ç‰¹æ€§æ¼”ç¤º
   - 6: è¿è¡Œæ‰€æœ‰æ¼”ç¤º
   - 0: é€€å‡º

### æ–¹å¼äºŒï¼šDocker é›†ç¾¤è¿è¡Œ

#### å‰ç½®è¦æ±‚

- Docker
- Docker Compose

#### æ­¥éª¤

1. **å¯åŠ¨ Spark é›†ç¾¤**
   ```bash
   docker-compose up -d
   ```

2. **æŸ¥çœ‹é›†ç¾¤çŠ¶æ€**
   - Spark Master UI: http://localhost:8080
   - Spark Worker UI: http://localhost:8082
   - Application UI: http://localhost:4040 (è¿è¡Œä½œä¸šæ—¶)

3. **ç¼–è¯‘é¡¹ç›®**
   ```bash
   mvn clean package
   ```

4. **æäº¤ä½œä¸šåˆ°é›†ç¾¤**
   ```bash
   docker exec -it spark-master /opt/spark/bin/spark-submit \
     --class com.spark.learning.SparkLearningMain \
     --master spark://spark-master:7077 \
     /opt/spark-apps/spark-learning-demo-1.0-SNAPSHOT.jar
   ```

5. **åœæ­¢é›†ç¾¤**
   ```bash
   docker-compose down
   ```

## ä½¿ç”¨è¯´æ˜

### äº¤äº’å¼èœå•

ç¨‹åºå¯åŠ¨åä¼šæ˜¾ç¤ºäº¤äº’å¼èœå•ï¼Œæ‚¨å¯ä»¥ï¼š

1. é€‰æ‹©å•ä¸ªæ¼”ç¤ºæ¨¡å—è¿è¡Œï¼ŒæŸ¥çœ‹ç‰¹å®šåŠŸèƒ½
2. é€‰æ‹©è¿è¡Œæ‰€æœ‰æ¼”ç¤ºï¼Œå®Œæ•´ä½“éªŒæ‰€æœ‰åŠŸèƒ½
3. æ¯ä¸ªæ¼”ç¤ºå®Œæˆåä¼šæš‚åœï¼ŒæŒ‰ Enter ç»§ç»­

### æŸ¥çœ‹è¾“å‡º

- **æ§åˆ¶å°è¾“å‡º**: æ‰€æœ‰æ¼”ç¤ºç»“æœéƒ½ä¼šç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°
- **æ•°æ®æ–‡ä»¶**: æ•°æ®è¯»å†™æ¼”ç¤ºä¼šåœ¨ `data/output/` ç›®å½•ä¸‹ç”Ÿæˆæ–‡ä»¶
- **æ—¥å¿—æ–‡ä»¶**: è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨ Spark æ—¥å¿—ç›®å½•

### ä¿®æ”¹é…ç½®

åœ¨ `SparkLearningMain.java` ä¸­å¯ä»¥ä¿®æ”¹ Spark é…ç½®ï¼š

```java
SparkConf conf = new SparkConf()
    .setAppName("Spark Learning Demo")
    .setMaster("local[*]")  // ä¿®æ”¹ä¸ºé›†ç¾¤åœ°å€
    .set("spark.sql.shuffle.partitions", "4")
    .set("spark.default.parallelism", "4");
```

## Docker é›†ç¾¤é…ç½®

### æœåŠ¡è¯´æ˜

- **spark-master**: Spark ä¸»èŠ‚ç‚¹
  - ç«¯å£ 8080: Web UI
  - ç«¯å£ 7077: é›†ç¾¤é€šä¿¡
  - ç«¯å£ 4040: åº”ç”¨ UI

- **spark-worker**: Spark å·¥ä½œèŠ‚ç‚¹
  - ç«¯å£ 8082: Web UI
  - é…ç½®: 2 æ ¸ CPU, 2GB å†…å­˜

### æ‰©å±• Worker èŠ‚ç‚¹

åœ¨ `docker-compose.yml` ä¸­æ·»åŠ æ›´å¤š worker:

```yaml
spark-worker-2:
  image: apache/spark:3.5.1
  container_name: spark-worker-2
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
    - SPARK_WORKER_CORES=2
    - SPARK_WORKER_MEMORY=2G
  ports:
    - "8083:8082"
  depends_on:
    spark-master:
      condition: service_healthy
```

## å­¦ä¹ è·¯å¾„

### åˆå­¦è€…

1. å…ˆè¿è¡Œ **RDD æ“ä½œæ¼”ç¤º**ï¼Œç†è§£ Spark çš„åŸºç¡€æ•°æ®ç»“æ„
2. å­¦ä¹  **DataFrame å’Œ Dataset æ¼”ç¤º**ï¼ŒæŒæ¡ç»“æ„åŒ–æ•°æ®å¤„ç†
3. ç†Ÿæ‚‰ **Spark SQL æ¼”ç¤º**ï¼Œå­¦ä¹  SQL æŸ¥è¯¢æ–¹å¼

### è¿›é˜¶å­¦ä¹ 

4. ç ”ç©¶ **æ•°æ®è¯»å†™æ¼”ç¤º**ï¼Œäº†è§£ä¸åŒæ•°æ®æ ¼å¼çš„ç‰¹ç‚¹
5. æ·±å…¥ **é«˜çº§ç‰¹æ€§æ¼”ç¤º**ï¼ŒæŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### å®è·µå»ºè®®

- ä¿®æ”¹ç¤ºä¾‹ä»£ç ï¼Œå°è¯•ä¸åŒçš„å‚æ•°
- ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œå®éªŒ
- é˜…è¯»ä»£ç æ³¨é‡Šï¼Œç†è§£æ¯ä¸ªæ“ä½œçš„å«ä¹‰
- å¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ€§èƒ½å·®å¼‚

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. é€‰æ‹©åˆé€‚çš„ç®—å­

- ä¼˜å…ˆä½¿ç”¨ `reduceByKey` è€Œä¸æ˜¯ `groupByKey`
- ä½¿ç”¨ `mapPartitions` ä»£æ›¿ `map` å¤„ç†æ‰¹é‡æ•°æ®
- åˆç†ä½¿ç”¨ `filter` å‡å°‘æ•°æ®é‡

### 2. ç¼“å­˜ç­–ç•¥

```java
// å¯¹é‡å¤ä½¿ç”¨çš„ RDD/DataFrame è¿›è¡Œç¼“å­˜
df.cache();
// æˆ–æŒ‡å®šå­˜å‚¨çº§åˆ«
df.persist(StorageLevel.MEMORY_AND_DISK());
```

### 3. åˆ†åŒºä¼˜åŒ–

```java
// å¢åŠ åˆ†åŒºæé«˜å¹¶è¡Œåº¦
df.repartition(100);
// å‡å°‘åˆ†åŒºé™ä½ shuffle å¼€é”€
df.coalesce(10);
```

### 4. å¹¿æ’­å˜é‡

```java
// å¯¹å¤§å‹æŸ¥æ‰¾è¡¨ä½¿ç”¨å¹¿æ’­å˜é‡
Broadcast<Map<String, String>> broadcast = sc.broadcast(lookupMap);
```

### 5. æ•°æ®å€¾æ–œå¤„ç†

- ä½¿ç”¨åŠ ç› (Salting) æŠ€æœ¯
- ä¸¤é˜¶æ®µèšåˆ
- åˆç†è®¾è®¡åˆ†åŒºé”®

## å¸¸è§é—®é¢˜

### Q1: å†…å­˜ä¸è¶³é”™è¯¯

**A**: è°ƒæ•´ Spark å†…å­˜é…ç½®ï¼š
```java
.set("spark.executor.memory", "4g")
.set("spark.driver.memory", "2g")
```

### Q2: åˆ†åŒºæ•°è¿‡å¤šæˆ–è¿‡å°‘

**A**: è°ƒæ•´åˆ†åŒºé…ç½®ï¼š
```java
.set("spark.sql.shuffle.partitions", "200")  // é»˜è®¤ 200
.set("spark.default.parallelism", "100")
```

### Q3: æ•°æ®å€¾æ–œå¯¼è‡´æ€§èƒ½é—®é¢˜

**A**: å‚è€ƒ `AdvancedFeaturesDemo` ä¸­çš„æ•°æ®å€¾æ–œå¤„ç†æ–¹æ³•

### Q4: Docker å®¹å™¨æ— æ³•å¯åŠ¨

**A**: 
- æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
- ç¡®ä¿ Docker æœ‰è¶³å¤Ÿçš„èµ„æºåˆ†é…
- æŸ¥çœ‹å®¹å™¨æ—¥å¿—: `docker logs spark-master`

## ç›¸å…³èµ„æº

- [Apache Spark å®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)
- [Spark Java API æ–‡æ¡£](https://spark.apache.org/docs/latest/api/java/index.html)
- [Spark SQL æŒ‡å—](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [æ€§èƒ½è°ƒä¼˜æŒ‡å—](https://spark.apache.org/docs/latest/tuning.html)

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è®¸å¯è¯

MIT License

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ GitHub Issue
- å‘é€é‚®ä»¶è‡³: [1647110340@qq.com]

---

**ç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼Happy Sparking! ğŸš€**

